{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDA Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing specialized packages: webdriver-manager, selenium, and wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\allen\\anaconda3\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\allen\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\allen\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\allen\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\allen\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\allen\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c https://conda.anaconda.org/conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing of useful packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import urllib\n",
    "import time\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "URL = \"https://open.fda.gov/data/downloads/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 102.0.5005\n",
      "[WDM] - Get LATEST chromedriver version for 102.0.5005 google-chrome\n",
      "[WDM] - About to download new driver from https://chromedriver.storage.googleapis.com/102.0.5005.61/chromedriver_win32.zip\n",
      "[WDM] - Driver has been saved in cache [C:\\Users\\Allen\\.wdm\\drivers\\chromedriver\\win32\\102.0.5005.61]\n"
     ]
    }
   ],
   "source": [
    "#The FDA website does not load the downloadable files unless you scroll to that area of the page first\n",
    "#Web-Scraping involves using the Selenium webdriver to open the site with Chrome, navigate to the\n",
    "    #needed area, and hit the correct buttons at the correct time\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.common.exceptions\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.google.com\")\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "button1 = driver.find_element(By.CLASS_NAME, \"button.bg-primary.clr-white\")\n",
    "button1.click()\n",
    "\n",
    "time.sleep(1)\n",
    " \n",
    "element_link=WebDriverWait(driver, 10).until(EC.presence_of_element_located(\n",
    "   (By.XPATH, '//*[@id=\"Medical Device Event\"]')))\n",
    "\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true)\", element_link)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "button2 = driver.find_element(By.XPATH, '//*[@id=\"Medical Device Event\"]/section/button')\n",
    "button2.click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "html = driver.execute_script(\"return document.getElementsByTagName('html')[0].innerHTML\")\n",
    "#print (html)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snip HTML to just the portion in question\n",
    "pattern = '1991(.*?)<li id=\"Medical Device PMA\">'\n",
    "substring = re.search(pattern, html).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Itemize links into array\n",
    "import lxml.html\n",
    "\n",
    "url_list = lxml.html.fromstring(substring)\n",
    "url_list = url_list.xpath('//a/@href')\n",
    "#print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2020\n",
    "end_year = 2021\n",
    "\n",
    "year_list = list(range(start_year, end_year+1))\n",
    "\n",
    "index_to_download = []\n",
    "\n",
    "for meh in year_list:\n",
    "    for bleh in range(0, len(url_list)):\n",
    "        if str(meh) in url_list[bleh]:\n",
    "            index_to_download.append(url_list.index(url_list[bleh]))\n",
    "\n",
    "index_count = len(index_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe ready\n"
     ]
    }
   ],
   "source": [
    "#Follow links in array to download/process ZIPs\n",
    "\n",
    "#Specify Download Path\n",
    "path = 'C:/Users/Allen/Documents/FDA'\n",
    "#pathinverse = 'C:\\Users\\Allen\\Documents\\FDA'\n",
    "import requests, zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "#Filter Data by Project Code, use \"All\" to include all data\n",
    "pcode = [\"CBK\",\"MNT\",\"MNS\",\"NOU\"]\n",
    "#pcode = [\"All\"]\n",
    "\n",
    "#Run loop, opening JSONs\n",
    "loopnumber = 0\n",
    "datamain = \"\"\n",
    "for snuh in index_to_download:\n",
    "    print('Download ' + (str(loopnumber+1)) + \" of \" + (str(index_count)) + \" started \")\n",
    "    url = url_list[snuh]\n",
    "    import requests, zipfile\n",
    "    req = requests.get(url)\n",
    "    print('Download ' + (str(loopnumber+1))+ \" completed \")\n",
    "    zipfile = zipfile.ZipFile(BytesIO(req.content))\n",
    "    #filename = \"FDA\" + str(snuh+1)\n",
    "    filename = \"FDAdata.json\"\n",
    "    for i, f in enumerate(zipfile.filelist):\n",
    "        f.filename = filename.format(i)\n",
    "        zipfile.extract(f)\n",
    "    print('File ' + (str(loopnumber+1))+ ' extracted')\n",
    "    data = json.load(open(r'C:\\Users\\Allen\\Documents\\FDA\\FDAdata.json'))\n",
    "    data = data[\"results\"]\n",
    "    datamain = data\n",
    "    if loopnumber == 0:\n",
    "        print('Creating Dataframe with JSON ' + (str(loopnumber+1)))\n",
    "        dfmain = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")\n",
    "        if pcode[0] != \"All\":\n",
    "            dfmain = dfmain[dfmain._device_report_product_code.isin(pcode)]\n",
    "        print('Dataframe Created')\n",
    "    else:\n",
    "        print('Appending Dataframe with JSON ' + (str(loopnumber+1)))\n",
    "        dfnew = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")\n",
    "        if pcode[0] != \"All\":\n",
    "            dfnew = dfnew[dfnew._device_report_product_code.isin(pcode)]\n",
    "        dfmain = pd.concat([dfmain, dfnew])\n",
    "        #dfmain.append(dfnew)\n",
    "        print('JSON ' + (str(loopnumber+1)) + ' appended')\n",
    "        \n",
    "    os.remove(path + \"/\" + filename)\n",
    "    loopnumber = loopnumber + 1\n",
    "print(\"Dataframe ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfmain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#dfmain.info()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Keep only relevant columns\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dfmain \u001b[38;5;241m=\u001b[39m \u001b[43mdfmain\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_device_report_product_code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_brand_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_generic_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_manufacturer_d_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype_of_report\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_number\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_source_code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_received\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmdr_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Rename columns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m dfmain\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrand_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneric_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanufacturer_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype_of_report\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_number\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_source_code\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_received\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmdr_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfmain' is not defined"
     ]
    }
   ],
   "source": [
    "#dfmain.info()\n",
    "\n",
    "#Keep only relevant columns\n",
    "dfmain = dfmain[[\"_device_report_product_code\",\"_brand_name\",\"_generic_name\",\"_manufacturer_d_name\",\"type_of_report\",\"report_number\",\"report_source_code\",\n",
    "                 \"date_received\",\"event_type\",\"mdr_text\"]]\n",
    "\n",
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')\n",
    "\n",
    "#Update MDR Text to only show the text narrative items--also lowercase the text\n",
    "newmdr = []\n",
    "for crag in dfmain[\"mdr_text\"]:\n",
    "    newmdr.append(''.join(re.findall(\"'text': .+?}\",str(crag))).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:])\n",
    "\n",
    "dfmain[\"mdr_text\"] = [x.lower() for x in newmdr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "#Remove stop words in MDR Text--we must remove null rows before this step can be done\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Add additional common words into stopwords\n",
    "stop_words.update([\"investigation\",\"still\",\"progress\",\"complete\",\"supplemental\",\"report\",\"filed\",\"device\",\"returned\",\"reported\",\n",
    "                  \"-\",\"due\",\"failure\",\"failed\",\"may\",\"ensure\",\"assures\",\"around\",\"met\",\"reports\",\"number\",\"per\",\"dated\"])\n",
    "\n",
    "dfmain = dfmain[dfmain['mdr_text'].notnull()]\n",
    "\n",
    "dfmain['mdr_text'] = dfmain['mdr_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "#dfmain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "no_dup = []\n",
    "\n",
    "#Following function found from internet\n",
    "def remove_duplicates(input):\n",
    " \n",
    "    # split input string separated by space\n",
    "    input = input.split(\" \")\n",
    " \n",
    "    # now create dictionary using counter method\n",
    "    # which will have strings as key and their\n",
    "    # frequencies as value\n",
    "    UniqW = Counter(input)\n",
    " \n",
    "    # joins two adjacent elements in iterable way\n",
    "    s = \" \".join(UniqW.keys())\n",
    "    return (s)\n",
    "\n",
    "dfmain['mdr_text_nodup'] = dfmain['mdr_text'].apply(remove_duplicates)\n",
    "\n",
    "dfmain['mdr_text_nodup'] = dfmain['mdr_text_nodup'].map(str)\n",
    "\n",
    "#dfmain.head()\n",
    "\n",
    "#dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Implement Word Cloud\n",
    "word_cloud = WordCloud(collocations = False, background_color = 'white').generate(str(dfmain[\"mdr_text_nodup\"]))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('foo.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create new date field with the days removed, for visualizations\n",
    "\n",
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "#dfmain['mdr_text_nodup'] = dfmain['mdr_text_nodup'].map(str)\n",
    "\n",
    "dfmain[\"year\"] = pd.to_datetime(dfmain[\"date_received\"], format = '%Y-%m-%d').dt.year\n",
    "dfmain[\"month\"] = pd.to_datetime(dfmain[\"date_received\"], format = '%Y-%m-%d').dt.month\n",
    "\n",
    "dfmain[\"plaindate\"] = pd.to_datetime(dfmain[['year', 'month']].assign(DAY=1))\n",
    "\n",
    "dfmain = dfmain.drop([\"year\",\"month\"], 1)\n",
    "\n",
    "#dfmain.head()\n",
    "\n",
    "dfmain.to_csv(r'data_vent.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create new dataframe--list of unique words in data, separated by month\n",
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "dfmain['mdr_text_nodup'] = dfmain['mdr_text_nodup'].map(str)\n",
    "\n",
    "dfword = pd.DataFrame()\n",
    "\n",
    "#Create unique list of dates to be used in the for loop\n",
    "datelist = dfmain[\"plaindate\"].unique()\n",
    "\n",
    "#Loop to create dataframe of unique words and the number of times they each appear\n",
    "wordnumbers = []\n",
    "for squid in datelist:\n",
    "    wordnumbers = Counter(' '.join(map(lambda l: ''.join(l), dfmain.loc[dfmain[\"plaindate\"] == squid][\"mdr_text_nodup\"])).split(\" \")) \n",
    "    dftemp = pd.DataFrame.from_dict(wordnumbers, orient='index').reset_index()\n",
    "    dftemp[\"Date\"] = squid\n",
    "    dfword = pd.concat([dfword, dftemp])\n",
    "    \n",
    "dfword.columns = [\"word\",\"counts\",\"date\"]\n",
    "\n",
    "dfword = dfword.drop(dfword[dfword.counts < 100].index)\n",
    "\n",
    "#dfword.head()\n",
    "dfword.to_csv(r'word_vent.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfword = pd.DataFrame.from_dict(wordnumbers, orient='index').reset_index()\n",
    "#dfmain.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "#Find # of times each unique word appears in dataframe\n",
    "Counter(' '.join(map(lambda l: ''.join(l), dfmain[\"mdr_text_nodup\"])).split(\" \"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "content = ' '.join(map(lambda snah: ' '.join(snah), dfmain[\"mdr_text_nodup\"]))\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\" \".join(dfmain[\"mdr_text_nodup\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Create new dataframe, list of unique words in the data\n",
    "content = ' '.join(map(lambda l: ''.join(l), dfmain[\"mdr_text_nodup\"]))\n",
    "tokens = word_tokenize(content)\n",
    "newwordsdf = pd.DataFrame(tokens)\n",
    "newwordsdf = newwordsdf.drop_duplicates()\n",
    "newwordsdf.columns = [\"word\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Populate columns of new dataframe --------- VERY SLOW, Maybe inaccurate\n",
    "sums = []\n",
    "for squid in newwordsdf[\"word\"]:\n",
    "    sums.append(sum(squid in snail for snail in dfmain[\"mdr_text_nodup\"]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "content.count(\"implant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmain.head()\n",
    "\n",
    "#dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newwordsdf.to_csv(r'words.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pcode = \"MDS\"\n",
    "#dftest = dfmain[dfmain[\"product_code\"] == pcode]\n",
    "#dftest.shape[0]\n",
    "dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = str(dfmain.iat[1,9])\n",
    "\n",
    "test2 = ''.join(re.findall(\"'text': .+?}\",test)).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:]\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "newmdr = []\n",
    "for crag in dfmain[\"mdr_text\"]:\n",
    "    newmdr.append(''.join(re.findall(\"'text': .+?}\",crag)).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:])\n",
    "\n",
    "dfmain[\"mdr_text\"] = [x.lower() for x in newmdr]\n",
    "dfmain.head(100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2 = dfmain.to_json(r'dataj.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2 = dfmain2 = pd.json_normalize(data,\n",
    "                  record_path = \"mdr_text\",\n",
    "                  errors = \"ignore\")\n",
    "dfmain2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2.to_csv(r'datajson.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2 = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Open and save the JSON file. Cull metadata, only keep results.\n",
    "data = json.load(open(r'C:\\Users\\Allen\\Documents\\FDA\\device-event-0001-of-0001.json'))\n",
    "data = data[\"results\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Normalize the JSON into a dataframe\n",
    "\n",
    "dfmain = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Keep only relevant columns\n",
    "dfmain = dfmain[[\"_device_report_product_code\",\"_brand_name\",\"_generic_name\",\"_manufacturer_d_name\",\"type_of_report\",\"report_number\",\"report_source_code\",\n",
    "                 \"date_received\",\"event_type\",\"mdr_text\"]]\n",
    "\n",
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input_Start_Date = datetime.date(2020, 1, 1)\n",
    "Input_End_Date = datetime.date(2020, 1, 31)\n",
    "\n",
    "Input_Start_Date"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Export to CSV\n",
    "dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_soup(URL):\n",
    "    return bs(requests.get(URL).text, 'html.parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_link = {}\n",
    "for link in get_soup(URL).findAll(\"a\", attrs={'href': re.compile(\".zip\")}):\n",
    "    file_link = link.get('href')\n",
    "print(file_link)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(link.text, 'wb') as file:\n",
    "    response = requests.get(DOMAIN + file_link)\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bs(requests.get(URL).text, 'html.parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "button = soup.find('button', {\"clr-white weight-700 bg-primary'})\n",
    "button['data-title']\n",
    "#'Huawei Matebook X Pro 53010CBS Laptop'\n",
    "button['data-button-title']\n",
    "#'Intel i5, 256GB SSD'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "html_content = requests.get(URL).text\n",
    "soup = bs(html_content, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "html = urllib.urlopen(URL).read()\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "your_data = list()\n",
    "\n",
    "for line in soup.findAll('span', attrs={'id': 'target_0'}):\n",
    "    your_data.append(line.text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "soup = bs(requests.get(URL).text, 'html.parser')\n",
    "your_data = list()\n",
    "for line in soup.findAll('span', attrs={'id': 'target_0'}):\n",
    "    your_data.append(line.text)\n",
    "\n",
    "print(your_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
