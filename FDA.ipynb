{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDA Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin by importing pandas, regular expressions, and numpy\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import urllib\n",
    "import time\n",
    "import os\n",
    "import string\n",
    "\n",
    "URL = \"https://open.fda.gov/data/downloads/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\allen\\anaconda3\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\allen\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\allen\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\allen\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: idna in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\allen\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\allen\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\allen\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 102.0.5005\n",
      "[WDM] - Get LATEST chromedriver version for 102.0.5005 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\Allen\\.wdm\\drivers\\chromedriver\\win32\\102.0.5005.61\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "#The FDA website does not load the doanloadable files unless you scroll to that area of the page first\n",
    "#Web-Scraping involves using the Selenium webdriver to open the site with Chrome, navigate to the\n",
    "    #needed area, and hit the correct buttons at the correct time\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.common.exceptions\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.google.com\")\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "button1 = driver.find_element(By.CLASS_NAME, \"button.bg-primary.clr-white\")\n",
    "button1.click()\n",
    "\n",
    "time.sleep(1)\n",
    " \n",
    "element_link=WebDriverWait(driver, 10).until(EC.presence_of_element_located(\n",
    "   (By.XPATH, '//*[@id=\"Medical Device Event\"]')))\n",
    "\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true)\", element_link)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "button2 = driver.find_element(By.XPATH, '//*[@id=\"Medical Device Event\"]/section/button')\n",
    "button2.click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "html = driver.execute_script(\"return document.getElementsByTagName('html')[0].innerHTML\")\n",
    "#print (html)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snip HTML to just the portion in question\n",
    "pattern = \"Hide(.*?)All other data\"\n",
    "substring = re.search(pattern, html).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Itemize links into array\n",
    "import lxml.html\n",
    "\n",
    "url_list = lxml.html.fromstring(substring)\n",
    "url_list = url_list.xpath('//a/@href')\n",
    "#print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2012\n",
    "end_year = 2012\n",
    "\n",
    "year_list = list(range(start_year, end_year+1))\n",
    "\n",
    "index_to_download = []\n",
    "\n",
    "for meh in year_list:\n",
    "    for bleh in range(0, len(url_list)):\n",
    "        if str(meh) in url_list[bleh]:\n",
    "            index_to_download.append(url_list.index(url_list[bleh]))\n",
    "\n",
    "index_count = len(index_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 1 of 8 started \n",
      "Download 1 completed \n",
      "File 1 extracted\n",
      "Creating Dataframe with JSON 1\n",
      "Dataframe Created\n",
      "Download 2 of 8 started \n",
      "Download 2 completed \n",
      "File 2 extracted\n",
      "Appending Dataframe with JSON 2\n",
      "JSON 2 appended\n",
      "Download 3 of 8 started \n",
      "Download 3 completed \n",
      "File 3 extracted\n",
      "Appending Dataframe with JSON 3\n",
      "JSON 3 appended\n",
      "Download 4 of 8 started \n",
      "Download 4 completed \n",
      "File 4 extracted\n",
      "Appending Dataframe with JSON 4\n",
      "JSON 4 appended\n",
      "Download 5 of 8 started \n",
      "Download 5 completed \n",
      "File 5 extracted\n",
      "Appending Dataframe with JSON 5\n",
      "JSON 5 appended\n",
      "Download 6 of 8 started \n",
      "Download 6 completed \n",
      "File 6 extracted\n",
      "Appending Dataframe with JSON 6\n",
      "JSON 6 appended\n",
      "Download 7 of 8 started \n",
      "Download 7 completed \n",
      "File 7 extracted\n",
      "Appending Dataframe with JSON 7\n",
      "JSON 7 appended\n",
      "Download 8 of 8 started \n",
      "Download 8 completed \n",
      "File 8 extracted\n",
      "Appending Dataframe with JSON 8\n",
      "JSON 8 appended\n",
      "Dataframe ready\n"
     ]
    }
   ],
   "source": [
    "#Follow links in array to download/process ZIPs\n",
    "\n",
    "#Specify Download Path\n",
    "path = 'C:/Users/Allen/Documents/FDA'\n",
    "#pathinverse = 'C:\\Users\\Allen\\Documents\\FDA'\n",
    "import requests, zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "#Filter Data by Project Code, use \"All\" to include all data\n",
    "#pcode = [\"GCY\",\"GAG\"]\n",
    "pcode = [\"All\"]\n",
    "\n",
    "#Run loop, opening JSONs\n",
    "loopnumber = 0\n",
    "datamain = \"\"\n",
    "for snuh in index_to_download:\n",
    "    print('Download ' + (str(loopnumber+1)) + \" of \" + (str(index_count)) + \" started \")\n",
    "    url = url_list[snuh]\n",
    "    import requests, zipfile\n",
    "    req = requests.get(url)\n",
    "    print('Download ' + (str(loopnumber+1))+ \" completed \")\n",
    "    zipfile = zipfile.ZipFile(BytesIO(req.content))\n",
    "    #filename = \"FDA\" + str(snuh+1)\n",
    "    filename = \"FDAdata.json\"\n",
    "    for i, f in enumerate(zipfile.filelist):\n",
    "        f.filename = filename.format(i)\n",
    "        zipfile.extract(f)\n",
    "    print('File ' + (str(loopnumber+1))+ ' extracted')\n",
    "    data = json.load(open(r'C:\\Users\\Allen\\Documents\\FDA\\FDAdata.json'))\n",
    "    data = data[\"results\"]\n",
    "    datamain = data\n",
    "    if loopnumber == 0:\n",
    "        print('Creating Dataframe with JSON ' + (str(loopnumber+1)))\n",
    "        dfmain = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")\n",
    "        if pcode[0] != \"All\":\n",
    "            dfmain = dfmain[dfmain._device_report_product_code.isin(pcode)]\n",
    "        print('Dataframe Created')\n",
    "    else:\n",
    "        print('Appending Dataframe with JSON ' + (str(loopnumber+1)))\n",
    "        dfnew = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")\n",
    "        if pcode[0] != \"All\":\n",
    "            dfnew = dfnew[dfnew._device_report_product_code.isin(pcode)]\n",
    "        dfmain = pd.concat([dfmain, dfnew])\n",
    "        #dfmain.append(dfnew)\n",
    "        print('JSON ' + (str(loopnumber+1)) + ' appended')\n",
    "        \n",
    "    os.remove(path + \"/\" + filename)\n",
    "    loopnumber = loopnumber + 1\n",
    "print(\"Dataframe ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmain.info()\n",
    "\n",
    "#Keep only relevant columns\n",
    "dfmain = dfmain[[\"_device_report_product_code\",\"_brand_name\",\"_generic_name\",\"_manufacturer_d_name\",\"type_of_report\",\"report_number\",\"report_source_code\",\n",
    "                 \"date_received\",\"event_type\",\"mdr_text\"]]\n",
    "\n",
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')\n",
    "\n",
    "#Update MDR Text to only show the text narrative items--also lowercase the text\n",
    "newmdr = []\n",
    "for crag in dfmain[\"mdr_text\"]:\n",
    "    newmdr.append(''.join(re.findall(\"'text': .+?}\",str(crag))).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:])\n",
    "\n",
    "dfmain[\"mdr_text\"] = [x.lower() for x in newmdr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmain.head()\n",
    "\n",
    "#dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pcode = \"MDS\"\n",
    "#dftest = dfmain[dfmain[\"product_code\"] == pcode]\n",
    "#dftest.shape[0]\n",
    "dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = str(dfmain.iat[1,9])\n",
    "\n",
    "test2 = ''.join(re.findall(\"'text': .+?}\",test)).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:]\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "newmdr = []\n",
    "for crag in dfmain[\"mdr_text\"]:\n",
    "    newmdr.append(''.join(re.findall(\"'text': .+?}\",crag)).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:])\n",
    "\n",
    "dfmain[\"mdr_text\"] = [x.lower() for x in newmdr]\n",
    "dfmain.head(100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2 = dfmain.to_json(r'dataj.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2 = dfmain2 = pd.json_normalize(data,\n",
    "                  record_path = \"mdr_text\",\n",
    "                  errors = \"ignore\")\n",
    "dfmain2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2.to_csv(r'datajson.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2 = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Open and save the JSON file. Cull metadata, only keep results.\n",
    "data = json.load(open(r'C:\\Users\\Allen\\Documents\\FDA\\device-event-0001-of-0001.json'))\n",
    "data = data[\"results\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Normalize the JSON into a dataframe\n",
    "\n",
    "dfmain = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Keep only relevant columns\n",
    "dfmain = dfmain[[\"_device_report_product_code\",\"_brand_name\",\"_generic_name\",\"_manufacturer_d_name\",\"type_of_report\",\"report_number\",\"report_source_code\",\n",
    "                 \"date_received\",\"event_type\",\"mdr_text\"]]\n",
    "\n",
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input_Start_Date = datetime.date(2020, 1, 1)\n",
    "Input_End_Date = datetime.date(2020, 1, 31)\n",
    "\n",
    "Input_Start_Date"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Export to CSV\n",
    "dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_soup(URL):\n",
    "    return bs(requests.get(URL).text, 'html.parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_link = {}\n",
    "for link in get_soup(URL).findAll(\"a\", attrs={'href': re.compile(\".zip\")}):\n",
    "    file_link = link.get('href')\n",
    "print(file_link)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(link.text, 'wb') as file:\n",
    "    response = requests.get(DOMAIN + file_link)\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bs(requests.get(URL).text, 'html.parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "button = soup.find('button', {\"clr-white weight-700 bg-primary'})\n",
    "button['data-title']\n",
    "#'Huawei Matebook X Pro 53010CBS Laptop'\n",
    "button['data-button-title']\n",
    "#'Intel i5, 256GB SSD'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "html_content = requests.get(URL).text\n",
    "soup = bs(html_content, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "html = urllib.urlopen(URL).read()\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "your_data = list()\n",
    "\n",
    "for line in soup.findAll('span', attrs={'id': 'target_0'}):\n",
    "    your_data.append(line.text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "soup = bs(requests.get(URL).text, 'html.parser')\n",
    "your_data = list()\n",
    "for line in soup.findAll('span', attrs={'id': 'target_0'}):\n",
    "    your_data.append(line.text)\n",
    "\n",
    "print(your_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
