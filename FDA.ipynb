{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDA Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing specialized packages: webdriver-manager, selenium, and wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c https://conda.anaconda.org/conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing of useful packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import urllib\n",
    "import time\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt, dates as mdates\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "%pylab inline\n",
    "import statistics\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "URL = \"https://open.fda.gov/data/downloads/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The FDA website does not load the downloadable files unless you scroll to that area of the page first\n",
    "#Web-Scraping involves using the Selenium webdriver to open the site with Chrome, navigate to the\n",
    "    #needed area, and hit the correct buttons at the correct time\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.common.exceptions\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.google.com\")\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#Get past the light screen\n",
    "button1 = driver.find_element(By.CLASS_NAME, \"button.bg-primary.clr-white\")\n",
    "button1.click()\n",
    "\n",
    "time.sleep(1)\n",
    " \n",
    "    #Scroll to the button for medical device events\n",
    "element_link=WebDriverWait(driver, 10).until(EC.presence_of_element_located(\n",
    "   (By.XPATH, '//*[@id=\"Medical Device Event\"]')))\n",
    "\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true)\", element_link)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#Click the medical device event button\n",
    "button2 = driver.find_element(By.XPATH, '//*[@id=\"Medical Device Event\"]/section/button')\n",
    "button2.click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#Retrieve the html code now that it displays the links we need\n",
    "html = driver.execute_script(\"return document.getElementsByTagName('html')[0].innerHTML\")\n",
    "#print (html)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snip HTML to just the portion in question\n",
    "pattern = '1991(.*?)<li id=\"Medical Device PMA\">'\n",
    "substring = re.search(pattern, html).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Itemize links into array\n",
    "import lxml.html\n",
    "\n",
    "url_list = lxml.html.fromstring(substring)\n",
    "url_list = url_list.xpath('//a/@href')\n",
    "#print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2011\n",
    "end_year = 2021\n",
    "\n",
    "year_list = list(range(start_year, end_year+1))\n",
    "\n",
    "index_to_download = []\n",
    "\n",
    "for meh in year_list:\n",
    "    for bleh in range(0, len(url_list)):\n",
    "        if str(meh) in url_list[bleh]:\n",
    "            index_to_download.append(url_list.index(url_list[bleh]))\n",
    "\n",
    "index_count = len(index_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Follow links in array to download/process ZIPs\n",
    "\n",
    "#Specify Download Path\n",
    "path = 'C:/Users/Allen/Documents/FDA'\n",
    "#pathinverse = 'C:\\Users\\Allen\\Documents\\FDA'\n",
    "import requests, zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "#Filter Data by Project Code, use \"All\" to include all data\n",
    "pcode = [\"DYE\",\"LWR\",\"MIE\",\"MWH\",\"NPX\",\"OHA\",\"PAL\",\"PAP\"]\n",
    "#pcode = [\"All\"]\n",
    "\n",
    "#Run loop, opening JSONs\n",
    "loopnumber = 0\n",
    "datamain = \"\"\n",
    "for snuh in index_to_download:\n",
    "    print('Download ' + (str(loopnumber+1)) + \" of \" + (str(index_count)) + \" started \")\n",
    "    url = url_list[snuh]\n",
    "    import requests, zipfile\n",
    "    req = requests.get(url)\n",
    "    print('Download ' + (str(loopnumber+1))+ \" completed \")\n",
    "    zipfile = zipfile.ZipFile(BytesIO(req.content))\n",
    "    #filename = \"FDA\" + str(snuh+1)\n",
    "    filename = \"FDAdata.json\"\n",
    "    for i, f in enumerate(zipfile.filelist):\n",
    "        f.filename = filename.format(i)\n",
    "        zipfile.extract(f)\n",
    "    print('File ' + (str(loopnumber+1))+ ' extracted')\n",
    "    data = json.load(open(r'C:\\Users\\Allen\\Documents\\FDA\\FDAdata.json'))\n",
    "    data = data[\"results\"]\n",
    "    datamain = data\n",
    "    if loopnumber == 0:\n",
    "        print('Creating Dataframe with JSON ' + (str(loopnumber+1)))\n",
    "        dfmain = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")\n",
    "        if pcode[0] != \"All\":\n",
    "            dfmain = dfmain[dfmain._device_report_product_code.isin(pcode)]\n",
    "        print('Dataframe Created')\n",
    "    else:\n",
    "        print('Appending Dataframe with JSON ' + (str(loopnumber+1)))\n",
    "        dfnew = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")\n",
    "        if pcode[0] != \"All\":\n",
    "            dfnew = dfnew[dfnew._device_report_product_code.isin(pcode)]\n",
    "        dfmain = pd.concat([dfmain, dfnew])\n",
    "        #dfmain.append(dfnew)\n",
    "        print('JSON ' + (str(loopnumber+1)) + ' appended')\n",
    "        \n",
    "    os.remove(path + \"/\" + filename)\n",
    "    loopnumber = loopnumber + 1\n",
    "print(\"Dataframe ready\")\n",
    "dfmain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmain.info()\n",
    "\n",
    "#Keep only relevant columns\n",
    "dfmain = dfmain[[\"_device_report_product_code\",\"_brand_name\",\"_generic_name\",\"_manufacturer_d_name\",\"type_of_report\",\"report_number\",\"report_source_code\",\n",
    "                 \"date_received\",\"event_type\",\"mdr_text\"]]\n",
    "\n",
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')\n",
    "\n",
    "#Update MDR Text to only show the text narrative items--also lowercase the text\n",
    "newmdr = []\n",
    "for crag in dfmain[\"mdr_text\"]:\n",
    "    newmdr.append(''.join(re.findall(\"'text': .+?}\",str(crag))).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:])\n",
    "\n",
    "dfmain[\"mdr_text\"] = [x.lower() for x in newmdr]\n",
    "dfmain.head()\n",
    "\n",
    "#dfmain.to_csv(r'data_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dfmain = pd.read_csv(\"data_long.csv\", encoding = 'unicode_escape')\n",
    "#dfmain['mdr_text'] = dfmain['mdr_text'].map(str)\n",
    "\n",
    "#Remove NA items\n",
    "dfmain.fillna('', inplace=True)\n",
    "\n",
    "#Prepare for lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Create function to tag words with what part of speech they are\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Create function to separate strings into words, assign parts of speech, and lemmatize accordingly\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "#Perform Lemmatization - this step can take some time\n",
    "#If the dataset is particularly large at this point, it may be worth skipping this step\n",
    "dfmain['mdr_text'] = dfmain['mdr_text'].apply(lambda x: lemmatize_sentence(x))\n",
    "\n",
    "#Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Add additional common words into stopwords\n",
    "stop_words.update([\"investigation\",\"still\",\"progress\",\"complete\",\"supplemental\",\"report\",\"filed\",\"device\",\"returned\",\"reported\",\n",
    "                  \"-\",\"due\",\"failure\",\"failed\",\"may\",\"ensure\",\"assures\",\"around\",\"met\",\"reports\",\"number\",\"per\",\"dated\",\"patient\",\n",
    "                  \"conclusion\",\"performed\",\"year\",\"years\",\"therefore\",\"submitted\",\"information\",\"received\",\"upon\",\"event\",\n",
    "                  \"events\",\"review\",\"required\",\"appropriate\",\"monitored\",\"monitor\",\"basis\",\"continue\",\"monthly\",\"trends\",\"completion\",\n",
    "                  \"additional\",\"months\",\"yet\",\"without\",\"history\",\"regarding\",\"cause\",\"record\",\"established\",\"accordingly\",\"procedure\",\n",
    "                  \"provided\",\"underwent\",\"reason\",\"evaluation\",\"time\",\"related\",\"product\",\"severed\",\"post\",\"return\",\"made\",\"also\",\n",
    "                  \"andor\",\"multiple\",\"remains\",\"determined\",\"replacement\",\"definitive\",\"issues\",\"however\",\"clinical\",\"factors\",\n",
    "                  \"effects\",\"release\",\"identified\",\"cannot\",\"reviewed\",\"would\",\"including\",\"available\",\"issued\",\"observation\",\n",
    "                  \"common\",\"action\",\"regard\",\"make\",\"future\",\"severe\",\"issue\",\"require\",\"receive\",\"month\",\"submit\",\"occur\",\n",
    "                  \"factor\",\"include\",\"provide\",\"remain\",\"contribute\",\"establish\",\"relate\",\"effect\",\"replace\",\"analysis\",\"via\",\n",
    "                  \"perform\",\"subject\",\"likely\",\"although\"])\n",
    "\n",
    "dfmain = dfmain[dfmain['mdr_text'].notnull()]\n",
    "\n",
    "dfmain['mdr_text'] = dfmain['mdr_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "dfmain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "no_dup = []\n",
    "\n",
    "#Following function found from internet\n",
    "def remove_duplicates(input):\n",
    " \n",
    "    # split input string separated by space\n",
    "    input = input.split(\" \")\n",
    " \n",
    "    # now create dictionary using counter method\n",
    "    # which will have strings as key and their\n",
    "    # frequencies as value\n",
    "    UniqW = Counter(input)\n",
    " \n",
    "    # joins two adjacent elements in iterable way\n",
    "    s = \" \".join(UniqW.keys())\n",
    "    return (s)\n",
    "\n",
    "dfmain['mdr_text_nodup'] = dfmain['mdr_text'].apply(remove_duplicates)\n",
    "\n",
    "dfmain['mdr_text_nodup'] = dfmain['mdr_text_nodup'].map(str)\n",
    "\n",
    "dfmain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Implement Word Cloud\n",
    "word_cloud = WordCloud(collocations = False, background_color = 'white').generate(str(dfmain[\"mdr_text_nodup\"]))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('foo.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create new date field with the days removed, for visualizations\n",
    "\n",
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "#dfmain['mdr_text_nodup'] = dfmain['mdr_text_nodup'].map(str)\n",
    "\n",
    "dfmain[\"year\"] = pd.to_datetime(dfmain[\"date_received\"], format = '%Y-%m-%d').dt.year\n",
    "dfmain[\"month\"] = pd.to_datetime(dfmain[\"date_received\"], format = '%Y-%m-%d').dt.month\n",
    "\n",
    "dfmain[\"plaindate\"] = pd.to_datetime(dfmain[['year', 'month']].assign(DAY=1))\n",
    "\n",
    "dfmain = dfmain.drop([\"year\",\"month\"], 1)\n",
    "\n",
    "dfmain.head()\n",
    "\n",
    "#dfmain.to_csv(r'data_organ.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfmain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create new dataframe--list of unique words in data, separated by month\n",
    "#dfmain = pd.read_csv(\"data_vent.csv\", encoding = 'unicode_escape')\n",
    "#dfmain['mdr_text_nodup'] = dfmain['mdr_text_nodup'].map(str)\n",
    "\n",
    "dfword = pd.DataFrame()\n",
    "\n",
    "#Create unique list of dates to be used in the for loop\n",
    "datelist = dfmain[\"plaindate\"].unique()\n",
    "\n",
    "#Loop to create dataframe of unique words and the number of times they each appear\n",
    "wordnumbers = []\n",
    "for squid in datelist:\n",
    "    wordnumbers = Counter(' '.join(map(lambda l: ''.join(l), dfmain.loc[dfmain[\"plaindate\"] == squid][\"mdr_text_nodup\"])).split(\" \")) \n",
    "    dftemp = pd.DataFrame.from_dict(wordnumbers, orient='index').reset_index()\n",
    "    dftemp[\"Date\"] = squid\n",
    "    dfword = pd.concat([dfword, dftemp])\n",
    "    \n",
    "dfword.columns = [\"word\",\"counts\",\"date\"]\n",
    "\n",
    "#Add column for % representation of each word during their time period\n",
    "def percentcalc(countcol, datecol):\n",
    "    perc = 0\n",
    "    perc = countcol / len(dfmain[dfmain[\"plaindate\"] == datecol])\n",
    "    return perc\n",
    "\n",
    "dfword[\"percent\"] = dfword.apply(lambda x: percentcalc(x['counts'],x['date']), axis = 1)\n",
    "\n",
    "dfword.head()\n",
    "dfword.to_csv(r'word.csv', index = False)\n",
    "dfmain.to_csv(r'main.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmain = pd.read_csv(\"main.csv\", encoding = 'unicode_escape')\n",
    "dfmain['mdr_text_nodup'] = dfmain['mdr_text_nodup'].map(str)\n",
    "dfword = pd.read_csv(\"word.csv\", encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a new column to show the change from month to month--this was a challenge to do in a way that was not too\n",
    "#time-intensive. Instead of trying to search for each previous month's value in the Dataframe and then performing the\n",
    "#subtracting calculation, we sort the dataset by word and then by date, and go through each item checking if the previous\n",
    "#line matches the word and the previous month. If these both match, the two are subtracted, but if not then the current\n",
    "#value must be the \"change\". This way we only go through the dataframe a single time, instead of once for each row.\n",
    "\n",
    "#I did attempt multiple searching algorithms but none of them could match the speed of this very dumbed-down approach.\n",
    "\n",
    "#Begin by sorting the dataset and removing a leftover junk column\n",
    "dfword = dfword.sort_values([\"word\",\"date\"]).reset_index()\n",
    "dfword.pop(\"index\")\n",
    "\n",
    "#Create a function to get the previous month off of a given string\n",
    "def getlastmonth(dateinquestion):\n",
    "    newdate = dateinquestion[5:7]\n",
    "    newdate = int(newdate) - 1\n",
    "    if newdate == 0:\n",
    "        newdate = 12\n",
    "    newdate = str(newdate)\n",
    "    if len(newdate) == 1:\n",
    "        newdate = \"0\" + newdate\n",
    "    newdate = dateinquestion[0:4] + \"-\" + newdate + dateinquestion[7:10]\n",
    "    return newdate\n",
    "\n",
    "#Create a function to check if the previous row in the dataset is the actual previous month for the word in question,\n",
    "#and then perform the subtraction if necessary to arrive at the change value\n",
    "def previous_values(indexitem):\n",
    "    if indexitem > 0:\n",
    "        if (dfword[\"date\"][indexitem-1] == getlastmonth(dfword[\"date\"][indexitem])) & (dfword[\"word\"][indexitem-1] == dfword[\"word\"][indexitem]):\n",
    "            return (dfword[\"counts\"][indexitem] - dfword[\"counts\"][indexitem-1])\n",
    "        else:\n",
    "            return dfword[\"counts\"][indexitem]\n",
    "    else:\n",
    "        return dfword[\"counts\"][indexitem]\n",
    "\n",
    "#Create the new column with the change value\n",
    "dfword[\"change\"] = dfword.apply(lambda x: previous_values(x.name), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfword.to_csv(r'wordheartchange.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with each month and and the number of reports for those months\n",
    "datelist = dfmain[\"plaindate\"].unique()\n",
    "\n",
    "dfline = pd.DataFrame()\n",
    "dfline[\"date\"] = datelist\n",
    "dfline[\"counts\"] = dfline[\"date\"].apply(lambda x: len(dfmain[dfmain[\"plaindate\"] == x]))\n",
    "\n",
    "#Set date column to the datetime type, and sort by date\n",
    "dfline[\"date\"] = pd.to_datetime(dfline[\"date\"])\n",
    "dfline = dfline.sort_values(by=\"date\")\n",
    "\n",
    "filter_date_start = \"2010-01-01\"\n",
    "filter_date_end = \"2022-12-31\"\n",
    "\n",
    "mask = (dfline['date'] >= filter_date_start) & (dfline['date'] <= filter_date_end)\n",
    "dfline = dfline.loc[mask]\n",
    "\n",
    "\n",
    "dfline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12, 9]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "formatter = mdates.DateFormatter(\"%Y-%m-%d\")\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "if len(dfline[\"date\"].unique()) > 12:\n",
    "    locator = mdates.MonthLocator(interval = 6)\n",
    "else:\n",
    "    locator = mdates.MonthLocator(interval = 1)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "plt.plot(dfline[\"date\"], dfline[\"counts\"])\n",
    "\n",
    "plt.suptitle(\"Device Reports\", fontsize = 20)\n",
    "plt.title(\"Product Codes: \" + \" \".join(dfmain[\"product_code\"].unique()))\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"# of Reports\")\n",
    "plt.xticks(rotation = 45)\n",
    "plt.axhline(y=dfline[\"counts\"].mean(), color='b', linestyle='--')\n",
    "plt.axhline(y=dfline[\"counts\"].mean()+dfline[\"counts\"].std(), color='y', linestyle='--')\n",
    "plt.axhline(y=dfline[\"counts\"].mean()+dfline[\"counts\"].std()*2, color='r', linestyle='--')\n",
    "plt.axhline(y=dfline[\"counts\"].mean()-dfline[\"counts\"].std(), color='y', linestyle='--')\n",
    "plt.axhline(y=dfline[\"counts\"].mean()-dfline[\"counts\"].std()*2, color='r', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(dfmain[\"product_code\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfline2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "\n",
    "#Create main window that everything else will fit into\n",
    "root = Tk()\n",
    "\n",
    "#Create a label on the root window--this does not display it yet\n",
    "#myLabel = Label(root, text = \"date\")\n",
    "\n",
    "#Put the label onto the root window - no formatting\n",
    "#myLabel.pack()\n",
    "\n",
    "#Put label onto the root window with formatting according to a grid shape\n",
    "#myLabel.grid(row=1, column = 0)\n",
    "\n",
    "#Create \"entry\" widget\n",
    "e = Entry(root, width = 20, bg = \"lightgray\", borderwidth = 3)\n",
    "e.pack()\n",
    "\n",
    "\n",
    "#Create what happens when you click on the button\n",
    "def myClick():\n",
    "    myLabel = Label(root, text = e.get())\n",
    "    myLabel.pack()\n",
    "    root.destroy()\n",
    "\n",
    "\n",
    "#Create button\n",
    "myButton = Button(root, text = \"Enter\", padx = 40, pady = 8, command = myClick)\n",
    "#Attach button\n",
    "myButton.pack()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create an event loop--a window which is constantly looping\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
