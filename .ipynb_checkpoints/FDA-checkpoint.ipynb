{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDA Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin by importing pandas, regular expressions, and numpy\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import urllib\n",
    "import time\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "URL = \"https://open.fda.gov/data/downloads/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c https://conda.anaconda.org/conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The FDA website does not load the doanloadable files unless you scroll to that area of the page first\n",
    "#Web-Scraping involves using the Selenium webdriver to open the site with Chrome, navigate to the\n",
    "    #needed area, and hit the correct buttons at the correct time\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.common.exceptions\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.google.com\")\n",
    "\n",
    "driver.get(URL)\n",
    "driver.maximize_window()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "button1 = driver.find_element(By.CLASS_NAME, \"button.bg-primary.clr-white\")\n",
    "button1.click()\n",
    "\n",
    "time.sleep(1)\n",
    " \n",
    "element_link=WebDriverWait(driver, 10).until(EC.presence_of_element_located(\n",
    "   (By.XPATH, '//*[@id=\"Medical Device Event\"]')))\n",
    "\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true)\", element_link)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "button2 = driver.find_element(By.XPATH, '//*[@id=\"Medical Device Event\"]/section/button')\n",
    "button2.click()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "html = driver.execute_script(\"return document.getElementsByTagName('html')[0].innerHTML\")\n",
    "#print (html)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snip HTML to just the portion in question\n",
    "pattern = \"Hide(.*?)All other data\"\n",
    "substring = re.search(pattern, html).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Itemize links into array\n",
    "import lxml.html\n",
    "\n",
    "url_list = lxml.html.fromstring(substring)\n",
    "url_list = url_list.xpath('//a/@href')\n",
    "#print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2020\n",
    "end_year = 2020\n",
    "\n",
    "year_list = list(range(start_year, end_year+1))\n",
    "\n",
    "index_to_download = []\n",
    "\n",
    "for meh in year_list:\n",
    "    for bleh in range(0, len(url_list)):\n",
    "        if str(meh) in url_list[bleh]:\n",
    "            index_to_download.append(url_list.index(url_list[bleh]))\n",
    "\n",
    "index_count = len(index_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Follow links in array to download/process ZIPs\n",
    "\n",
    "#Specify Download Path\n",
    "path = 'C:/Users/Allen/Documents/FDA'\n",
    "#pathinverse = 'C:\\Users\\Allen\\Documents\\FDA'\n",
    "import requests, zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "#Filter Data by Project Code, use \"All\" to include all data\n",
    "#pcode = [\"GCY\",\"GAG\"]\n",
    "pcode = [\"All\"]\n",
    "\n",
    "#Run loop, opening JSONs\n",
    "loopnumber = 0\n",
    "datamain = \"\"\n",
    "for snuh in index_to_download:\n",
    "    print('Download ' + (str(loopnumber+1)) + \" of \" + (str(index_count)) + \" started \")\n",
    "    url = url_list[snuh]\n",
    "    import requests, zipfile\n",
    "    req = requests.get(url)\n",
    "    print('Download ' + (str(loopnumber+1))+ \" completed \")\n",
    "    zipfile = zipfile.ZipFile(BytesIO(req.content))\n",
    "    #filename = \"FDA\" + str(snuh+1)\n",
    "    filename = \"FDAdata.json\"\n",
    "    for i, f in enumerate(zipfile.filelist):\n",
    "        f.filename = filename.format(i)\n",
    "        zipfile.extract(f)\n",
    "    print('File ' + (str(loopnumber+1))+ ' extracted')\n",
    "    data = json.load(open(r'C:\\Users\\Allen\\Documents\\FDA\\FDAdata.json'))\n",
    "    data = data[\"results\"]\n",
    "    datamain = data\n",
    "    if loopnumber == 0:\n",
    "        print('Creating Dataframe with JSON ' + (str(loopnumber+1)))\n",
    "        dfmain = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")\n",
    "        if pcode[0] != \"All\":\n",
    "            dfmain = dfmain[dfmain._device_report_product_code.isin(pcode)]\n",
    "        print('Dataframe Created')\n",
    "    else:\n",
    "        print('Appending Dataframe with JSON ' + (str(loopnumber+1)))\n",
    "        dfnew = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")\n",
    "        if pcode[0] != \"All\":\n",
    "            dfnew = dfnew[dfnew._device_report_product_code.isin(pcode)]\n",
    "        dfmain = pd.concat([dfmain, dfnew])\n",
    "        #dfmain.append(dfnew)\n",
    "        print('JSON ' + (str(loopnumber+1)) + ' appended')\n",
    "        \n",
    "    os.remove(path + \"/\" + filename)\n",
    "    loopnumber = loopnumber + 1\n",
    "print(\"Dataframe ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmain.info()\n",
    "\n",
    "#Keep only relevant columns\n",
    "dfmain = dfmain[[\"_device_report_product_code\",\"_brand_name\",\"_generic_name\",\"_manufacturer_d_name\",\"type_of_report\",\"report_number\",\"report_source_code\",\n",
    "                 \"date_received\",\"event_type\",\"mdr_text\"]]\n",
    "\n",
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')\n",
    "\n",
    "#Update MDR Text to only show the text narrative items--also lowercase the text\n",
    "newmdr = []\n",
    "for crag in dfmain[\"mdr_text\"]:\n",
    "    newmdr.append(''.join(re.findall(\"'text': .+?}\",str(crag))).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:])\n",
    "\n",
    "dfmain[\"mdr_text\"] = [x.lower() for x in newmdr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "#Remove stop words in MDR Text--we must remove null rows before this step can be done\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "dfmain = dfmain[dfmain['mdr_text'].notnull()]\n",
    "\n",
    "dfmain['mdr_text'] = dfmain['mdr_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "dfmain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "no_dup = []\n",
    "\n",
    "#Following function found from internet\n",
    "def remove_duplicates(input):\n",
    " \n",
    "    # split input string separated by space\n",
    "    input = input.split(\" \")\n",
    " \n",
    "    # now create dictionary using counter method\n",
    "    # which will have strings as key and their\n",
    "    # frequencies as value\n",
    "    UniqW = Counter(input)\n",
    " \n",
    "    # joins two adjacent elements in iterable way\n",
    "    s = \" \".join(UniqW.keys())\n",
    "    return (s)\n",
    "\n",
    "dfmain['mdr_text_nodup'] = dfmain['mdr_text'].apply(remove_duplicates)\n",
    "\n",
    "dfmain.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Word Cloud\n",
    "word_cloud = WordCloud(collocations = False, background_color = 'white').generate(str(dfmain[\"mdr_text_nodup\"]))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmain.head()\n",
    "\n",
    "dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pcode = \"MDS\"\n",
    "#dftest = dfmain[dfmain[\"product_code\"] == pcode]\n",
    "#dftest.shape[0]\n",
    "dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = str(dfmain.iat[1,9])\n",
    "\n",
    "test2 = ''.join(re.findall(\"'text': .+?}\",test)).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:]\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')\n",
    "\n",
    "newmdr = []\n",
    "for crag in dfmain[\"mdr_text\"]:\n",
    "    newmdr.append(''.join(re.findall(\"'text': .+?}\",crag)).translate(str.maketrans('', '', string.punctuation)).replace(\"text\",\" - \")[4:])\n",
    "\n",
    "dfmain[\"mdr_text\"] = [x.lower() for x in newmdr]\n",
    "dfmain.head(100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain = pd.read_csv(\"data.csv\", encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2 = dfmain.to_json(r'dataj.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2 = dfmain2 = pd.json_normalize(data,\n",
    "                  record_path = \"mdr_text\",\n",
    "                  errors = \"ignore\")\n",
    "dfmain2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2.to_csv(r'datajson.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfmain2 = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Open and save the JSON file. Cull metadata, only keep results.\n",
    "data = json.load(open(r'C:\\Users\\Allen\\Documents\\FDA\\device-event-0001-of-0001.json'))\n",
    "data = data[\"results\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Normalize the JSON into a dataframe\n",
    "\n",
    "dfmain = pd.json_normalize(data,\n",
    "                  record_path = \"device\",\n",
    "                  meta = [\"report_number\",\"report_source_code\",\"date_received\",\"event_type\",\"type_of_report\",\"mdr_text\"],\n",
    "                  record_prefix = \"_\",\n",
    "                  errors = \"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Keep only relevant columns\n",
    "dfmain = dfmain[[\"_device_report_product_code\",\"_brand_name\",\"_generic_name\",\"_manufacturer_d_name\",\"type_of_report\",\"report_number\",\"report_source_code\",\n",
    "                 \"date_received\",\"event_type\",\"mdr_text\"]]\n",
    "\n",
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Rename columns\n",
    "dfmain.columns = [\"product_code\",\"brand_name\",\"generic_name\",\"manufacturer_name\",\"type_of_report\",\"report_number\",\n",
    "                        \"report_source_code\",\"date_received\",\"event_type\",\"mdr_text\"]\n",
    "\n",
    "#Update date column to date format\n",
    "dfmain[\"date_received\"] = pd.to_datetime(dfmain[\"date_received\"])\n",
    "\n",
    "#Remove brackets from type of report column\n",
    "dfmain['type_of_report'] = dfmain['type_of_report'].str.join(', ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input_Start_Date = datetime.date(2020, 1, 1)\n",
    "Input_End_Date = datetime.date(2020, 1, 31)\n",
    "\n",
    "Input_Start_Date"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Export to CSV\n",
    "dfmain.to_csv(r'data.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_soup(URL):\n",
    "    return bs(requests.get(URL).text, 'html.parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_link = {}\n",
    "for link in get_soup(URL).findAll(\"a\", attrs={'href': re.compile(\".zip\")}):\n",
    "    file_link = link.get('href')\n",
    "print(file_link)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(link.text, 'wb') as file:\n",
    "    response = requests.get(DOMAIN + file_link)\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bs(requests.get(URL).text, 'html.parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "button = soup.find('button', {\"clr-white weight-700 bg-primary'})\n",
    "button['data-title']\n",
    "#'Huawei Matebook X Pro 53010CBS Laptop'\n",
    "button['data-button-title']\n",
    "#'Intel i5, 256GB SSD'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "html_content = requests.get(URL).text\n",
    "soup = bs(html_content, \"lxml\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "html = urllib.urlopen(URL).read()\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "your_data = list()\n",
    "\n",
    "for line in soup.findAll('span', attrs={'id': 'target_0'}):\n",
    "    your_data.append(line.text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "soup = bs(requests.get(URL).text, 'html.parser')\n",
    "your_data = list()\n",
    "for line in soup.findAll('span', attrs={'id': 'target_0'}):\n",
    "    your_data.append(line.text)\n",
    "\n",
    "print(your_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
